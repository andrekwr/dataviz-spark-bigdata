{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"Teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.sequenceFile(\"pages/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total de documentos na base\n",
    "total_docs = rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções de tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantas ocorrencias da palavra em todos documentos\n",
    "def conta_palavras(item):\n",
    "    texto = item[1]\n",
    "    palavras = texto.strip().split()\n",
    "    return [(palavra.lower(),1) for palavra in palavras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conta quantos docs cada palavra aparece\n",
    "def conta_docs(item):\n",
    "    texto = item[1]\n",
    "    palavras = texto.strip().split()\n",
    "    return [(palavra.lower(),1) for palavra in set(palavras)]\n",
    "rdd_docs_word = rdd.flatMap(conta_docs).reduceByKey(lambda x,y: x + y).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtra palavras que aparecem em certa quantidade de docs\n",
    "doc_freq_min = 10\n",
    "doc_freq_max = 0.7 * total_docs\n",
    "def filtra(item):\n",
    "    contagem = item[1]\n",
    "    return (contagem < doc_freq_max) and (contagem > doc_freq_min)\n",
    "\n",
    "#RDD com quantos docs cada palavra aparece considerando intervalo limite de 5 a 0.7*total_documentos\n",
    "rdd_docs_filtrado = rdd_docs_word.filter(filtra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise do vocabulário comum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pons', 3.4117759853793284),\n",
       " ('r$', 0.5932673172690647),\n",
       " ('//}', 3.3274550996792924),\n",
       " ('à', 0.32189609502544525),\n",
       " ('e-mail', 0.43047524320596725),\n",
       " ('bom', 0.8289013117853762),\n",
       " ('joaquim', 1.6987657237630354),\n",
       " ('cantores', 2.5086859983873846),\n",
       " ('região', 0.881576287176246),\n",
       " ('*', 0.864528870032381)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calcula idf de cada palavra filtrada\n",
    "def computa_idf(item):\n",
    "    palavra, contagem = item\n",
    "    idf = math.log10(total_docs / contagem)\n",
    "    return (palavra, idf)\n",
    "rdd_idf = rdd_docs_filtrado.map(computa_idf)\n",
    "rdd_idf.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise do vocabulário específico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrar RDDs para selecionar conjuntos com cada palavra definida.\n",
    "rdd_oreo = rdd.filter(lambda x: \"oreo\" in x[1]) \n",
    "rdd_negresco = rdd.filter(lambda x: \"negresco\" in x[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cálculo da frequência para os RDDs dos dois conjuntos.\n",
    "rdd_freq_oreo = rdd_oreo.flatMap(conta_palavras).reduceByKey(lambda x,y: x + y).map(lambda x: (x[0], math.log10(1 + x[1]))).cache()\n",
    "rdd_freq_negresco = rdd_negresco.flatMap(conta_palavras).reduceByKey(lambda x,y: x + y).map(lambda x: (x[0], math.log10(1 + x[1]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intersecção dos dois conjuntos de frequência.\n",
    "rdd_inter = rdd_freq_oreo.intersection(rdd_freq_negresco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tabela com 100 palavras mais relevantes onde os dois itens aparecem conjuntamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 6.0 failed 1 times, most recent failure: Lost task 6.0 in stage 6.0 (TID 48, LAPTOP-EFUL12DG, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 2030, in combine\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 147, in load_stream\n    yield self._read_with_length(stream)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 169, in _read_with_length\n    obj = stream.read(length)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 2030, in combine\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 147, in load_stream\n    yield self._read_with_length(stream)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 169, in _read_with_length\n    obj = stream.read(length)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3ac19efdb141>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Calcula 100 palavras mais relevantes em conjunto às palavras escolhidas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrdd_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd_inter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd_idf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtakeOrdered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrdd_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pages/rdd_all.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtakeOrdered\u001b[1;34m(self, num, key)\u001b[0m\n\u001b[0;32m   1388\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mheapq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnsmallest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1390\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mheapq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnsmallest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \"\"\"\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 6.0 failed 1 times, most recent failure: Lost task 6.0 in stage 6.0 (TID 48, LAPTOP-EFUL12DG, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 2030, in combine\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 147, in load_stream\n    yield self._read_with_length(stream)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 169, in _read_with_length\n    obj = stream.read(length)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 595, in process\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\rdd.py\", line 2030, in combine\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 147, in load_stream\n    yield self._read_with_length(stream)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 169, in _read_with_length\n    obj = stream.read(length)\n  File \"C:\\Users\\AndreWeber\\AppData\\Local\\Programs\\Python\\Python38\\lib\\socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "#Calcula 100 palavras mais relevantes em conjunto às palavras escolhidas\n",
    "rdd_all = rdd_inter.join(rdd_idf).map(lambda x: (x[0], x[1][0]*x[1][1])).takeOrdered(1, key=lambda x: -x[1])\n",
    "rdd_all.saveAsTextFile(\"pages/rdd_all.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tabela com 100 palavras mais relevantes sem a presença de \"negresco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('riachuelo', 8.378763843447578),\n",
       " ('soil', 7.882439711900808),\n",
       " ('teens', 6.914087807944809),\n",
       " ('great.', 6.683526884745895),\n",
       " ('i’m', 6.587384961277829),\n",
       " ('responda', 6.58399381824705),\n",
       " ('cool.', 6.554107335323577),\n",
       " ('forward', 6.48867612719286),\n",
       " ('rchlo', 6.384720996588554),\n",
       " ('thank', 6.328807730846682),\n",
       " ('i’ll', 6.319928024771057),\n",
       " (\"levi's\", 6.313044381390977),\n",
       " ('appreciate', 6.310778353563317),\n",
       " ('cachecol,', 6.3100349048863285),\n",
       " ('really', 6.30764695153316),\n",
       " ('article.', 6.29904898393505),\n",
       " ('you!', 6.264579233323951),\n",
       " ('more.', 6.2321536658164005),\n",
       " ('nfl', 6.208421463312811),\n",
       " ('suéter', 6.196256862404943),\n",
       " ('grow', 6.180697312642146),\n",
       " ('writing.', 6.139480882653706),\n",
       " ('education,', 6.129436513946753),\n",
       " ('bahls', 6.091145457070974),\n",
       " ('jerseys', 6.085719813604868),\n",
       " ('cells', 6.064057145561836),\n",
       " ('enjoyed', 6.047435310109896),\n",
       " ('loans', 5.993716764971986),\n",
       " ('{{item.name}}', 5.991479887551308),\n",
       " ('íntima', 5.990699037868668),\n",
       " ('this.', 5.988677304928174),\n",
       " ('helpful', 5.946616750233836),\n",
       " ('excellent', 5.926156346167428),\n",
       " ('i’d', 5.921067598195036),\n",
       " ('brides', 5.902947769471932),\n",
       " ('perfume', 5.850905801408288),\n",
       " ('awesome.', 5.847106763963664),\n",
       " ('pool', 5.827498079425903),\n",
       " ('lot', 5.7887070829108715),\n",
       " ('again.', 5.770557417494633),\n",
       " ('salgueiro,', 5.767441919533278),\n",
       " ('(p', 5.745598442921946),\n",
       " ('v:', 5.743262827887056),\n",
       " ('surfwear', 5.74241271420548),\n",
       " ('meninos', 5.732852326942641),\n",
       " ('certainly', 5.708928469558002),\n",
       " ('educational', 5.702325883248314),\n",
       " ('useful', 5.684958846249689),\n",
       " ('very', 5.648846265763274),\n",
       " ('definitely', 5.629849590431892),\n",
       " ('fantastic', 5.626617075592149),\n",
       " ('disse:', 5.616157297647762),\n",
       " ('gorro', 5.603223183056943),\n",
       " ('defensive', 5.594996726756914),\n",
       " ('weblog', 5.582094842044518),\n",
       " ('his', 5.564962394313246),\n",
       " ('dating', 5.561447103597917),\n",
       " ('wow,', 5.5578492350871445),\n",
       " ('football', 5.554461309830634),\n",
       " ('stumbled', 5.5348726849753245),\n",
       " ('sizecalça', 5.534378578624579),\n",
       " ('glad', 5.5219694275335),\n",
       " ('payday', 5.521784278539867),\n",
       " ('ak', 5.517126943207969),\n",
       " ('could', 5.516538416979971),\n",
       " ('bride', 5.5095873210724795),\n",
       " ('inspire-se:', 5.502457231963849),\n",
       " ('environmental', 5.499203240330479),\n",
       " ('it’s', 5.486862745329789),\n",
       " ('truly', 5.4694833483926635),\n",
       " ('meninasver', 5.467047557226703),\n",
       " ('colônias', 5.460682838577504),\n",
       " (\"carter's\", 5.450629597059949),\n",
       " ('cm)', 5.442918126765245),\n",
       " ('you’re', 5.439628221501062),\n",
       " ('looking', 5.4356546098389),\n",
       " ('blusões', 5.413826256994435),\n",
       " ('think', 5.411003677883677),\n",
       " ('here.', 5.403134557916676),\n",
       " ('he', 5.394151229890007),\n",
       " ('eau', 5.38916232187422),\n",
       " ('bufantes', 5.385462972469937),\n",
       " ('sent', 5.365749565946229),\n",
       " ('reprodução/', 5.359056210433279),\n",
       " ('patients', 5.35241326651704),\n",
       " ('givenchy', 5.350679728073704),\n",
       " ('would', 5.344563426101736),\n",
       " ('suéteres', 5.342372752264284),\n",
       " ('m·a·c', 5.340958786765194),\n",
       " ('loved', 5.339733218081051),\n",
       " ('bookmarked', 5.327590588162454),\n",
       " ('cbd', 5.327421291308122),\n",
       " ('you’ve', 5.3271171089332565),\n",
       " ('posta', 5.325873448931105),\n",
       " ('shiseido', 5.323736937885473),\n",
       " ('economic', 5.3230541321391796),\n",
       " ('casaco', 5.321561909765721),\n",
       " (\"'s\", 5.32000098427693),\n",
       " ('treated', 5.31289923618334),\n",
       " ('guerlain', 5.3121193834784135)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calcula 100 palavras mais relevantes com apenas do conjunto Oreo\n",
    "rdd_freq_oreo.subtractByKey(rdd_freq_negresco).join(rdd_idf).map(lambda x: (x[0], x[1][0]*x[1][1])).takeOrdered(100, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tabela com 100 palavras mais relevantes sem a presença de \"oreo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mármore', 5.833777920042529),\n",
       " ('juliana,', 5.772869418743419),\n",
       " ('granito', 5.6992696159764415),\n",
       " ('manteiga,', 5.158187549175862),\n",
       " ('mármores', 4.513494469386839),\n",
       " ('lavatório', 4.4543124815496755),\n",
       " ('juliana!', 4.280390686285018),\n",
       " ('granitos', 4.15973029572628),\n",
       " ('manteiga.', 4.145702536465033),\n",
       " ('confeiteiro', 4.14284416144329),\n",
       " ('margarina', 3.836646023263132),\n",
       " ('geladeira,', 3.6866352216031464),\n",
       " ('soleira', 3.602200958617797),\n",
       " ('açucar', 3.4766460532793557),\n",
       " ('receita,', 3.382433644093701),\n",
       " ('merengue', 3.3195451504692186),\n",
       " ('corante', 3.311231520847154),\n",
       " ('cotar', 3.2634298068495045),\n",
       " ('confeitar', 3.151696965500766),\n",
       " ('testei', 3.073622004679498),\n",
       " ('chocolate,', 3.0406377649871903),\n",
       " ('juliana.', 3.028768783613926),\n",
       " ('travertino', 3.0049913826856485),\n",
       " ('hahahaha.', 2.939861798913858),\n",
       " ('marmoraria', 2.864255767058688),\n",
       " ('sal,', 2.847771453817274),\n",
       " ('açúcar,', 2.8397800759331178),\n",
       " ('balcão', 2.787945755397189),\n",
       " ('\",\"', 2.7585634716039844),\n",
       " ('m2', 2.7109503211079957),\n",
       " ('líquido,', 2.6895107353686436),\n",
       " ('batedor', 2.6738153422992306),\n",
       " ('pingadeira', 2.654877749051812),\n",
       " ('sal', 2.601536863156728),\n",
       " ('senão', 2.584190305973133),\n",
       " ('cobertura,', 2.5827963585353704),\n",
       " ('ode', 2.4871084948352795),\n",
       " ('receitas,', 2.485054433877187),\n",
       " ('ana!', 2.457935943701222),\n",
       " ('230g', 2.457935943701222),\n",
       " ('mármore,', 2.44567543313156),\n",
       " ('condensado,', 2.4315228731325065),\n",
       " ('bruna,', 2.4072252035697757),\n",
       " ('líquido.', 2.386233746283174),\n",
       " ('receita!', 2.365105192073611),\n",
       " ('marmore', 2.365105192073611),\n",
       " ('honestamente', 2.3637856809200617),\n",
       " ('receita.', 2.3254765988420645),\n",
       " ('honestamente,', 2.292027783359066),\n",
       " ('iniciais.', 2.292027783359066),\n",
       " ('adorando', 2.291748938409565),\n",
       " ('140g', 2.2616465671884294),\n",
       " ('lareira', 2.2576061404205108),\n",
       " ('bastante!', 2.2475250063266974),\n",
       " ('servir,', 2.2087200594387766),\n",
       " ('oii', 2.196814266195251),\n",
       " ('dúvida.', 2.1962937138114924),\n",
       " ('gel,', 2.192473310163596),\n",
       " ('geladeira.', 2.1743181379195895),\n",
       " ('derreter', 2.1743181379195895),\n",
       " ('refinado', 2.1519868787542644),\n",
       " ('duvida', 2.1266987976421925),\n",
       " ('eh', 2.1266987976421925),\n",
       " ('mto', 2.1266987976421925),\n",
       " ('errado?', 2.1065809457071443),\n",
       " ('vez!', 2.098029427652297),\n",
       " ('amido', 2.083181264542732),\n",
       " ('margarina,', 2.073470930361908),\n",
       " ('bati', 2.073470930361908),\n",
       " ('francieli', 2.073470930361908),\n",
       " ('supermercado.', 2.019179189075951),\n",
       " ('bjinhos', 2.019179189075951),\n",
       " ('acrescente', 1.993832691128893),\n",
       " ('pitada', 1.9806210813104088),\n",
       " ('usar.', 1.9804469098915096),\n",
       " ('mole,', 1.9742452734913423),\n",
       " ('beijos!!', 1.9742452734913423),\n",
       " ('começei', 1.9742452734913423),\n",
       " ('gostoso,', 1.9639072005450646),\n",
       " ('creme.', 1.9608335474847276),\n",
       " ('aqui…', 1.9480763182704353),\n",
       " ('errar,', 1.9359126678173257),\n",
       " ('rsrs…', 1.9359126678173257),\n",
       " ('fazer,', 1.9322538013668555),\n",
       " ('receitas.', 1.9281531217082202),\n",
       " ('carol!', 1.9242898062478064),\n",
       " ('(:', 1.9242898062478064),\n",
       " ('colorida,', 1.9242898062478064),\n",
       " ('blog!!!', 1.9242898062478064),\n",
       " ('quente.', 1.9233725691146168),\n",
       " ('creme,', 1.9131616871759576),\n",
       " ('dúvida:', 1.9094666806000733),\n",
       " ('omitir', 1.9024879058935047),\n",
       " ('cobrir', 1.8858938782830827),\n",
       " ('condicionado.', 1.8823647999548836),\n",
       " ('bata', 1.8648674996362387),\n",
       " ('resolvi', 1.8591896370874492),\n",
       " ('conservar', 1.8556711010330067),\n",
       " ('pq', 1.8538216217536132),\n",
       " ('certo!', 1.84082469243938)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calcula 100 palavras mais relevantes com apenas do conjunto Negresco\n",
    "rdd_freq_negresco.subtractByKey(rdd_freq_oreo).join(rdd_idf).map(lambda x: (x[0], x[1][0]*x[1][1])).takeOrdered(100, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
